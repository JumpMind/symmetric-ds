
== Manage

=== Nodes

One _node_ is associated with one relational database. A node manages the
synchronization of data to and from that database.  A node communicates with its 
database via JDBC.  Nodes communicate with each other over HTTP/S using the protocol described 
by <<Data Format>>.

ifdef::pro[]
Nodes can be added and managed in the Nodes screen under the Manage tab.

image::manage/manage-nodes.png[]
endif::pro[]
    
    
==== Add Node

include::manage/node-add.ad[]

==== Control

include::manage/node-control.ad[]

==== Registration

include::manage/node-registration.ad[]
		
==== Initial Loads

include::manage/node-initial-load.ad[]

==== Send

include::manage/node-send.ad[]
	
=== Jobs

Work done by SymmetricDS is initiated by jobs. Jobs are tasks that are
started and scheduled by a job manager. Jobs are enabled by the
`start.{name}.job`
property. Most jobs are enabled by default. The frequency at which a job
runs in controlled by one of two properties:
`job.{name}.period.time.ms`
or
`job.{name}.cron`
. If a valid cron property exists in the configuration, then it will be
used to schedule the job. Otherwise, the job manager will attempt to use
the period.time.ms property.


The frequency of jobs can be configured in either the engines properties
file or in
<<PARAMETER>>
. When managed in
<<PARAMETER>>
the frequency properties can be changed in the registration server and
when the updated settings sync to the nodes in the system the job
manager will restart the jobs at the new frequency settings.


SymmetricDS utilizes Spring's CRON support, which includes seconds as
the first parameter. This differs from the typical Unix-based
implementation, where the first parameter is usually minutes. For
example,
`*/15 * * * * *`
means every 15 seconds, not every 15 minutes. See
http://static.springsource.org/spring/docs/3.0.x/javadoc-api/org/springframework/scheduling/support/CronSequenceGenerator.html"[Spring's
documentation]
for more details.


Some jobs cannot be run in parallel against a single node. When running
on a cluster these jobs use the
<<LOCK>>
table to get an exclusive semaphore to run the job. In order to use this
table the
`cluster.lock.enabled`
must be set to true.

The three main jobs in SymmetricDS are the route, push and
pull jobs. The route job decides what captured data changes should be
sent to which nodes. It also decides what captured data changes should
be transported and loaded together in a batch. The push and pull jobs
are responsible for initiating HTTP communication with linked nodes to
push or pull data changes that have been routed. 
		
==== Route Job

After data is captured in the
<<DATA>>
table, it is routed to specific nodes in batches by the
_Route Job_
. It is a single background task that inserts into
<<DATA_EVENT>>
and
<<OUTGOING_BATCH>>
.


The job processes each enabled channel, one at a time, collecting a list
of data ids from
<<DATA>>
which have not been routed (see
<<Data Gaps>>
for much more detail about this step), up to a limit specified by the
channel configuration (
`max_data_to_route`
, on
<<CHANNEL>>
). The data is then batched based on the
`batch_algorithm`
defined for the channel. Note that, for the
_default_
and
_transactional_
algorithm, there may actually be more than
`max_data_to_route`
included depending on the transaction boundaries. The mapping of data to
specific nodes, organized into batches, is then recorded in
<<OUTGOING_BATCH>>
with a status of "RT" in each case (representing the fact that the Route
Job is still running). Once the routing algorithms and batching are
completed, the batches are organized with their corresponding data ids
and saved in
<<DATA_EVENT>>
. Once
<<DATA_EVENT>>
is updated, the rows in
<<OUTGOING_BATCH>>
are updated to a status of New "NE".


The route job will respect the
`max_batch_size`
on
<<OUTGOING_BATCH>>
. If the max batch size is reached before the end of a database
transaction and the batch algorithm is set to something other than
_nontransactional_
the batch may exceed the specified max size.


The route job delegates to a router defined by the
`router_type`
and configured by the
`router_expression`
in the
<<ROUTER>>
table. Each router that has a
_source_node_group_id_
that matches the current node's source node group id and is linked to
the
<<TRIGGER>>
that captured the data gets an opportunity to choose a list of nodes the
data should be sent to. Data can only be routed to nodes that belong to
the router's
_target_node_group_id_
		
==== Data Gaps

On the surface, the first Route Job step of collecting unrouted data ids
seems simple: assign sequential data ids for each data row as it's
inserted and keep track of which data id was last routed and start from
there. The difficulty arises, however, due to the fact that there can be
multiple transactions inserting into
<<DATA>>
simultaneously. As such, a given section of rows in the
<<DATA>>
table may actually contain "gaps" in the data ids when the Route Job is
executing. Most of these gaps are only temporarily and fill in at some
point after routing and need to be picked up with the next run of the
Route Job. Thus, the Route Job needs to remember to route the filled-in
gaps. Worse yet, some of these gaps are actually permanent and result
from a transaction that is rolled back for some reason. In this case,
the Route Job must continue to watch for the gap to fill in and, at some
point, eventually gives up and assumes the gap is permanent and can be
skipped. All of this must be done in some fashion that guarantees that
gaps are routed when they fill in while also keeping routing as
efficient as possible.


SymmetricDS handles the issue of data gaps by making use of a table,
<<DATA_GAP>>
, to record gaps found in the data ids. In fact, this table completely
defines the entire range of data that can be routed at any point in time.
For a brand new instance of SymmetricDS, this table is empty and
SymmetricDS creates a gap starting from data id of zero and ending with
a very large number (defined by
`routing.largest.gap.size`
). At the start of a Route Job, the list of valid gaps (gaps with status
of 'GP') is collected, and each gap is evaluated in turn. If a gap is
sufficiently old (as defined by
`routing.stale.dataid.gap.time.ms`
, the gap is marked as skipped (status of 'SK') and will no longer be
evaluated in future Route Jobs (note that the 'last' gap (the one with
the highest starting data id) is never skipped). If not skipped, then
<<DATA_EVENT>>
is searched for data ids present in the gap. If one or more data ids is
found in
<<DATA_EVENT>>
, then the current gap is marked with a status of OK, and new gap(s) are
created to represent the data ids still missing in the gap's range. This
process is done for all gaps. If the very last gap contained data, a new
gap starting from the highest data id and ending at (highest data id +
`routing.largest.gap.size`
) is then created. This process has resulted in an updated list of gaps
which may contain new data to be routed.
		
==== Push and Pull Jobs for Database changes

After database-change data is routed, it awaits transport to the target nodes. Transport
can occur when a client node is configured to pull data or when the host
node is configured to push data. These events are controlled by the
_push_
and the
_pull jobs_
. When the
`start.pull.job`
SymmetricDS property is set to
`true`
, the frequency that data is pulled is controlled by the
`job.pull.period.time.ms`
. When the
`start.push.job`
SymmetricDS property is set to
`true`
, the frequency that data is pushed is controlled by the
`job.push.period.time.ms`
.


Data is extracted by channel from the source database's
<<DATA>>
table at an interval controlled by the
`extract_period_millis`
column on the
<<CHANNEL>>
table. The
`last_extract_time`
is always recorded, by channel, on the
<<NODE_CHANNEL_CTL>>
table for the host node's id. When the Pull and Push Job run, if the
extract period has not passed according to the last extract time, then
the channel will be skipped for this run. If the
`extract_period_millis`
is set to zero, data extraction will happen every time the jobs run.


The maximum number of batches to extract per synchronization is
controlled by
_max_batch_to_send_
on the
<<CHANNEL>>
table. There is also a setting that controls the max number of bytes to
send in one synchronization. If SymmetricDS has extracted the more than
the number of bytes configured by the
`transport.max.bytes.to.sync`
parameter, then it will finish extracting the current batch and finish
synchronization so the client has a chance to process and acknowledge the
"big" batch. This may happen before the configured max number of batches
has been reached.


Both the push and pull jobs can be configured to push and pull multiple
nodes in parallel. In order to take advantage of this the
`pull.thread.per.server.count`
or
`push.thread.per.server.count`
should be adjusted (from their default value of 10) to the number to the
number of concurrent push/pulls you want to occur per period on each
SymmetricDS instance. Push and pull activity is recorded in the
<<NODE_COMMUNICATION>>
table. This table is also used to lock push and pull activity across
multiple servers in a cluster.


SymmetricDS also provides the ability to configure windows of time when
synchronization is allowed. This is done using the
<<NODE_GROUP_CHANNEL_WND>>
table. A list of allowed time windows can be specified for a node group
and a channel. If one or more windows exist, then data will only be
extracted and transported if the time of day falls within the window of
time specified. The configured times are always for the target node's
local time. If the
`start_time`
is greater than the
`end_time`
, then the window crosses over to the next day.


All data loading may be disabled by setting the
`dataloader.enable`
property to false. This has the effect of not allowing incoming
synchronizations, while allowing outgoing synchronizations. All data
extractions may be disabled by setting the
`dataextractor.enable`
property to false. These properties can be controlled by inserting into
the root server's
<<PARAMETER>>
table. These properties affect every channel with the exception of the
'config' channel.

Node communication over HTTP is represented in the
following figure. 


.Node Communication
image::seq-node-communication.gif[]

==== File Sync Push and Pull Jobs

The File Sync Push and Pull jobs (introduced in version 3.5) are responsible for synchronizing file changes.
These jobs work with batches on the `filesync` channel and create ZIP files of changed files
to be sent and applied on other nodes.
The parameters `job.file.sync.push.period.time.ms` and `job.file.sync.pull.period.time.ms`
control how often the jobs runs, which default to every 60 seconds.
See also <<Jobs>> and <<File Synchronization>>.
		
==== File System Tracker Job
		
The File System Tracker job (introduced in version 3.5) is responsible for monitoring and
recording the events of files being created, modified, or deleted.
It records the current state of files to the <<FILE_SNAPSHOT>> table.
The parameter `job.file.sync.tracker.cron` controls how often the job runs,
which defaults to every 5 minutes.
See also <<Jobs>> and <<File Synchronization>>.
		
==== Sync Triggers Job
		
SymmetricDS examines the current configuration, corresponding database
triggers, and the underlying tables to determine if database triggers
need created or updated. The change activity is recorded on the
<<TRIGGER_HIST>>
table with a reason for the change. The following reasons for a change
are possible:
		
[horizontal]		
N::  New trigger that has not been created before
S::  Schema changes in the table were detected
C::  Configuration changes in Trigger
T::  Trigger was missing
		
A configuration entry in Trigger without any history in Trigger Hist
results in a new trigger being created (N). The Trigger Hist stores a
hash of the underlying table, so any alteration to the table causes the
trigger to be rebuilt (S). When the
`last_update_time`
is changed on the Trigger entry, the configuration change causes the
trigger to be rebuilt (C). If an entry in Trigger Hist is missing the
corresponding database trigger, the trigger is created (T).
		
The process of examining triggers and rebuilding them is automatically
run during startup and each night by the SyncTriggersJob. The user can
also manually run the process at any time by invoking the
`syncTriggers()`
method over JMX.
		
==== Purge Jobs
		
Purging is the act of cleaning up captured data that is no longer needed
in SymmetricDS's runtime tables. Data is purged through delete
statements by the
_Purge Job_
. Only data that has been successfully synchronized will be purged.
Purged tables include:

* <<DATA>>
* <<DATA_EVENT>>
* <<OUTGOING_BATCH>>
* <<INCOMING_BATCH>>
* <<DATA_GAP>>
* <<NODE_HOST_STATS>>
* <<NODE_HOST_CHANNEL_STATS>>
* <<NODE_HOST_JOB_STATS>>
		
		
The purge job is enabled by the
`start.purge.job`
SymmetricDS property. The timing of the three purge jobs (incoming,
outgoing, and data gaps) is controlled by a cron expression as specified
by the following properties:
`job.purge.outgoing.cron`
,
`job.purge.incoming.cron`
, and
`job.purge.datagaps.cron`
. The default is
`0 0 0 * * *`
, or once per day at midnight.

Two retention period properties indicate how much history SymmetricDS
will retain before purging. The
`purge.retention.minutes`
property indicates the period of history to keep for synchronization
tables. The default value is 5 days. The
`statistic.retention.minutes`
property indicates the period of history to keep for statistics. The
default value is also 5 days.

The purge properties should be adjusted according to how
much data is flowing through the system and the amount of storage space
the database has. For an initial deployment it is recommended that the
purge properties be kept at the defaults, since it is often helpful to
be able to look at the captured data in order to triage problems and
profile the synchronization patterns. When scaling up to more nodes, it
is recommended that the purge parameters be scaled back to 24 hours or
less. 

=== Installed Triggers

TODO 

=== Outgoing Loads

TODO

=== Outgoing Batches
		
By design, whenever SymmetricDS encounters an issue with a synchronization, the batch containing the error is marked as being in
an error state, and all subsequent batches _for that particular channel to that particular node_ are held and not
synchronized until the error batch is resolved.  SymmetricDS will retry the batch in error until the situation creating the
error is resolved (or the data for the batch itself is changed).
       	
Analyzing and resolving issues can take place on the outgoing or incoming side.  The techniques for analysis are slightly different in
the two cases, however, due to the fact that the node with outgoing batch data also has the data and data events associated with the batch in
the database.  On the incoming node, however, all that is available is the incoming batch header and data present in an incoming error table.
		
==== Analyzing the Issue

The first step in analyzing the cause of a failed batch is to locate information about the data in the batch, starting with
<<OUTGOING_BATCH>>
.  To locate batches in error, use:
   
[source, sql]
----
select * from sym_outgoing_batch where error_flag=1;
----

Several useful pieces of information are available from this query:

* The batch number of the failed batch, available in column `BATCH_ID`.
* The node to which the batch is being sent, available in column `NODE_ID`.
* The channel to which the batch belongs, available in column `CHANNEL_ID`.
   All subsequent batches on this channel to this node will be held until the error condition is resolved.
* The specific data id in the batch which is causing the failure, available in column `FAILED_DATA_ID`.
* Any SQL message, SQL State, and SQL Codes being returned during the synchronization attempt, available in columns `SQL_MESSAGE`,
   `SQL_STATE`, and `SQL_CODE`, respectively.
	       
NOTE: Using the `error_flag` on the batch table, as shown above, is more reliable than using the
   `status` column.  The status column can change from 'ER' to a different status temporarily as
   the batch is retried.
	       
NOTE: The query above will also show you any recent batches that
   were originally in error and were changed to be manually skipped.  See the end of <<Outgoing Batches>> for more details.

To get a full picture of the batch, you can query for information representing the complete
list of all data changes associated with the failed batch by joining
<<DATA>> and <<DATA_EVENT>>, such as:
   
[source, sql]
----
select * from sym_data where data_id in
   (select data_id from sym_data_event where batch_id='XXXXXX');
----
where XXXXXX is the batch id of the failing batch.
	      
This query returns a wealth of information about each data change in a batch, including:

* The table involved in each data change, available in column `TABLE_NAME`,
* The event type (Update [U], Insert [I], or Delete [D]), available in column `EVENT_TYPE`,
* A comma separated list of the new data and (optionally) the old data, available in columns `ROW_DATA` and
	       `OLD_DATA`, respectively.
* The primary key data, available in column `PK_DATA`
* The channel id, trigger history information, transaction id if available, and other information.

More importantly, if you narrow your query to just the failed data id you can determine the exact data change that is causing the failure:
[source, sql]
----
select * from sym_data where data_id in
    (select failed_data_id from sym_outgoing_batch where batch_id='XXXXX'
    and node_id='YYYYY');
----
where XXXXXX is the batch id and YYYYY is the node id of the batch that is failing.
	       
The queries above usually yield enough information to be able to determine why a
   particular batch is failing. Common reasons a batch might be failing include:

* The schema at the destination has a column that is not nullable yet the source
has the column defined as nullable and a data change was sent with the column as null.
* A foreign key constraint at the destination is preventing an insertion or update, which could be caused from
data being deleted at the destination or the foreign key constraint is not in place at the source.
* The data size of a column on the destination is smaller than the data size in the source, and data that
is too large for the destination has been synced.

==== Resolving the Issue
           
Once you have decided upon the cause of the issue, you'll have to decide the best course of action to fix the issue.  If, for example,
the problem is due to a database schema mismatch, one possible solution would be to alter the destination database
in such a way that the SQL error no longer occurs.  Whatever approach you take to remedy the issue, once you have
made the change, on the next push or pull SymmetricDS will retry the batch
and the channel's data will start flowing again.

If you have instead decided that the batch itself is wrong, or does not need synchronized, or you wish to remove a
particular data change from a batch, you do have the option of changing the data associated with the batch directly.

WARNING: Be cautious when using the following two approaches to resolve synchronization issues.  By far, the
best approach to solving a synchronization error is to resolve what is truly causing the
error at the destination database.  Skipping a batch or removing a data id as discussed below should be your
solution of last resort, since doing so results in differences between the source and destination databases.

Now that you've read the warning, if you _still_ want to change the batch
data itself, you do have several options, including:
            
* Causing SymmetricDS to skip the batch completely.  This is accomplished by setting the
batch's status to 'OK', as in:
[source, sql]  
----              
update sym_outgoing_batch set status='OK' where batch_id='XXXXXX'
----                
where XXXXXX is the failing batch. On the next pull or push, SymmetricDS will skip this batch since
it now thinks the batch has already been synchronized.  Note that you can still distinguish between successful
batches and ones that you've artificially marked as 'OK', since the `error_flag` column on
the failed batch will still be set to '1' (in error).

* Removing the failing data id from the batch by deleting the corresponding row in <<DATA_EVENT>>.
Eliminating the data id from the list of data ids in the batch will cause future synchronization attempts
of the batch to no longer include that particular data change as part of the batch.  For example:
                  
[source, sql]
delete from sym_data_event where batch_id='XXXXXX' and data_id='YYYYYY'
where XXXXXX is the failing batch and YYYYYY is the data id to longer be included in the batch.

=== Incoming Batches

==== Analyzing the Issue
	      
Analysis using an incoming batch is different than that of outgoing batches.  For incoming batches, you will rely on two tables,
<<INCOMING_BATCH>> and <<INCOMING_ERROR>>.

The first step in analyzing the cause of an incoming failed batch is to locate information about the batch, starting with
<<INCOMING_BATCH>>
.  To locate batches in error, use:

[source, sql]
----
select * from sym_incoming_batch where error_flag=1;
----

Several useful pieces of information are available from this query:

* The batch number of the failed batch, available in column `BATCH_ID`.  Note that this is the batch number of the
outgoing batch on the outgoing node.
* The node the batch is being sent from, available in column `NODE_ID`.
* The channel to which the batch belongs, available in column `CHANNEL_ID`.
All subsequent batches on this channel from this node will be held until the error condition is resolved.
* The data_id that was being processed when the batch failed, available in column `FAILED_DATA_ID`.
* Any SQL message, SQL State, and SQL Codes being returned during the synchronization attempt, available in columns `SQL_MESSAGE`,
`SQL_STATE`, and `SQL_CODE`, respectively.

For incoming batches, we do not have data and data event entries in the database we can query.
We do, however, have a table, <<INCOMING_ERROR>>, which provides some information about the batch.

[source, sql]
----
select * from sym_incoming_error
where batch_id='XXXXXX' and node_id='YYYYY';
----
where XXXXXX is the batch id and YYYYY is the node id of the failing batch.

This query returns a wealth of information about each data change in a batch, including:

* The table involved in each data change, available in column `TARGET_TABLE_NAME`,
* The event type (Update [U], Insert [I], or Delete [D]), available in column `EVENT_TYPE`,
* A comma separated list of the new data and (optionally) the old data, available in columns `ROW_DATA` and
`OLD_DATA`, respectively,
* The column names of the table, available in column `COLUMN_NAMES`,
* The primary key column names of the table, available in column `PK_COLUMN_NAMES`,
	       
==== Resolving the Issue
           
For batches in error, from the incoming side you'll also have to decide the best course of action to fix the issue.
Incoming batch errors _that are in conflict_ can by fixed by taking advantage of two columns in <<INCOMING_ERROR>> which are examined each time
batches are processed.  The first column, `resolve_data` if filled in will be used in place of `row_data`.
The second column, `resolve_ignore` if set will cause this particular data item to be ignored and batch processing to continue.  This is the same
two columns used when a manual conflict resolution strategy is chosen, as discussed in <<Conflicts>>.
      
      
=== Staging Area
		
SymmetricDS creates temporary extraction and data load files with the CSV payload of a synchronization when
the value of the `stream.to.file.threshold.bytes` SymmetricDS property has been reached.  Before reaching the threshold, files
are streamed to/from memory.  The default threshold value is 32,767 bytes. This feature may be turned off by setting the `stream.to.file.enabled`
property to false.
        
SymmetricDS creates these temporary files in the directory specified by the `java.io.tmpdir` Java System property.
       
The location of the temporary directory may be changed by setting the Java System property passed into the Java program at startup.  For example,
[source, cli]
----
-Djava.io.tmpdir=/home/.symmetricds/tmp
----   

=== Processes

TODO

=== Pull Threads

TODO 

=== Push Threads

TODO

=== JVM Properties

TODO

=== JVM Threads

TODO

=== Logging
	   
The standalone SymmetricDS installation uses http://logging.apache.org/log4j/1.2/index.html[Log4J] for logging.  The configuration file is `conf/log4j.xml`.
The `log4j.xml` file has hints as to what logging can be enabled for useful, finer-grained logging.

There is a command line option to turn on preconfigured debugging levels.  When the `--debug` option is used the `conf/debug-log4j.xml` is used instead of log4j.xml.

SymmetricDS proxies all of its logging through http://www.slf4j.org/[SLF4J].  When deploying to an application server or if Log4J is not
being leveraged, then the general rules for for SLF4J logging apply.
