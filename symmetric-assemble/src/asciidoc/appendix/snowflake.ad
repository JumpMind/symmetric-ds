=== Snowflake

Use `symadmin module install snowflake` to install driver files, or copy your own files into the `lib` sub-directory.

==== Setup

Snowflake supports running as a write only node in SymmetricDS.  See <<Write Only Node>> for details on setting up a write only node in SymmetricDS.
Snowflake can also be set up to capture changes using log-mining.


ifdef::pro[]
Setup the Snowflake node by using the <<Add Node,Connect Database>> wizard and selecting Snowflake as the type.  

image::images/appendix/snowflake-database-settings.png[]

After hitting next you can setup advanced options for your Snowflake node.

endif::pro[]

ifndef::pro[] 

.Example properties to setup a Snowflake write only node
----
load.only=true
target.db.driver=net.snowflake.client.jdbc.SnowflakeDriver
target.db.url=jdbc:snowflake://<account_name>.snowflakecomputing.com/?db=<database_name>
target.db.user=<snowflake_user>
target.db.password=<snowflake_password>
----

endif::pro[]

==== Permissions
.Log Miner Permissions
If the tables that are being tracked for change data capture using log mining are already created, each table needs to be altered
to add change data tracking to it. An example DDL statement to turn this on is shown:
[source,SQL]
----
ALTER TABLE TABLE1 SET CHANGE_TRACKING=TRUE;
----

Permissions must also be added to the database user ID that is used to access the Snowflake database. Assuming that you
wanted to use a role called SYMMETRIC and a user called SYMMETRIC, here are the set of permissions that must be set in
order for SymmetricDS to successfully run the log mining (called Streams in Snowflake) functionality (replace <warehouse>,
<database>, and <schema> with real values):
[source,SQL]
----
CREATE ROLE SYMMETRIC;
GRANT USAGE ON WAREHOUSE <warehouse> TO ROLE SYMMETRIC;
GRANT USAGE ON DATABASE <database> TO ROLE SYMMETRIC;
GRANT USAGE ON SCHEMA <database>.<schema> TO ROLE SYMMETRIC;
GRANT CREATE STREAM ON SCHEMA <database>.<schema> TO ROLE SYMMETRIC;
GRANT SELECT ON ALL TABLES IN DATABASE <database> TO ROLE SYMMETRIC;
GRANT CREATE TABLE ON SCHEMA <database>.<schema> TO ROLE SYMMETRIC;
GRANT CREATE FILE FORMAT ON SCHEMA SYMMETRIC_TEST.SYMMETRIC_TEST TO ROLE SYMMETRIC;
CREATE USER SYMMETRIC PASSWORD = 'symmetric' DEFAULT_ROLE = 'SYMMETRIC';
GRANT ROLE SYMMETRIC TO USER SYMMETRIC;
----

==== Bulk Loading



ifndef::pro[] 
===== Setup reload channels for bulk loading.

Update any reload channels that will be used on the table triggers that will capture changes and send them to snowflake by setting the column data_loader_type to 'bulk'.  It is also recommended to increase the batch size so that larger CSV files will be processed instead of the default size on reloads of 10,000 rows.


.Example SQL to setup the main reload channel to use bulk and also update the batch sizes.
[source, SQL]
----
update sym_channel set data_loader_type='bulk', max_batch_size=500000 where channel_id='reload'
----
endif::pro[]

If you have SymmetricDS create the tables to synchronize into Snowflake, SymmetricDS creates the tables with a cluster
column specification based on the primary key of the table. If you do not want SymmetricDS to create the clustered columns,
set the following property in the symmetric-server.properties file in the conf directory and restart the instance:
[source,properties]
----
snowflake.cluster.primary.keys=false
----

If you do not have SymmetricDS create the tables, and you want clustered columns to be specified, you will have to define
the cluster specification yourself. Here is an example statement of a clustered column specification for a primary key column
called ID on table TABLE1:
[source,SQL]
----
ALTER TABLE TABLE1 CLUSTER BY (ID);
----

When defining the triggers, make sure to specify the source catalog (which would be the Snowflake database value) and the source
schema for each table.

===== Choose a bulk load storage option

SymmetricDS will create and send CSV files to the a desired storage location (see below) as part of the load.  Once the CSV files have been uploaded to a selected storage area Snowflake's COPY INTO command will be used to load the data into Snowflake.  Once the COPY INTO has completed SymmetricDS will also remove the CSV file from the storage container.

.There are currently 3 supported storage options to stage the CSV files prior to loading into Snowflake
* Snowflake Managed (internal storage)
* AWS: S3 
* Azure: Storage Account

      
SNOWFLAKE MANAGED:: Use a Snowflake managed internal stage.
ifdef::pro[]
image::images/appendix/snowflake-advanced-settings-snowflake-managed.png[]
endif::pro[]
ifndef::pro[] 
[source, properties]
----
snowflake.staging.type=SNOWFLAKE_INTERNAL
snowflake.internal.stage.name=<defaults to symmetricds>
----
endif::pro[]


AWS S3:: Use an existing AWS S3 cloud storage.
ifdef::pro[]
image::images/appendix/snowflake-advanced-settings-aws-s3.png[]
endif::pro[]
ifndef::pro[] 
[source, properties]
----
snowflake.staging.type=AWS_S3
cloud.bulk.load.s3.bucket=
cloud.bulk.load.s3.access.key=
cloud.bulk.load.s3.secret.key=
----
endif::pro[]


AZURE Storage Account:: Use an existing Azure Storage Account.
ifdef::pro[]
image::images/appendix/snowflake-advanced-settings-azure.png[]
endif::pro[]
ifndef::pro[] 
[source, properties]
----
snowflake.staging.type=AZURE
cloud.bulk.load.azure.account.name=<storage account name>
cloud.bulk.load.azure.account.key=
cloud.bulk.load.azure.blob.container=<defaults to symmetricds>
cloud.bulk.load.azure.sas.token=
----
endif::pro[]


