
=== Redshift

Redshift is a managed data warehouse in the cloud from Amazon. Version 1.0 of Redshift is based on PostgreSQL 8.0, 
with some features modified or removed. SymmetricDS supports Redshift as a target platform where data can be loaded, 
but it does not support data capture. However, the initial load and reload functions are implemented, so it is 
possible to query rows from Redshift tables and send them to another database.

While Redshift started with PostgreSQL 8.0, there are some important differences from PostgreSQL. Redshift does 
not support constraints, indexes, functions, triggers, or sequences. Primary keys, foreign keys, and unique indexes 
can be defined on tables, but they are informational metadata that are not enforced by the system. When using the 
default data loader with SymmetricDS, it will enforce primary keys, either defined in the database or with the sync 
keys features, by checking if a row exists before attempting an insert. However, the bulk loader does not perform 
this check. The data types supported are smallint, integer, bigint, decimal, real, double precision, boolean, char, 
varchar, date, and timestamp.

A data loader named "redshift_bulk" is a bulk loader that can be set for a channel to improve loading performance. 
Instead of sending individual SQL statements to the database, it creates a comma separated value (CSV) file, uploads 
the object to Amazon S3, and uses the COPY statement to load it. The COPY command appends the new data to any existing 
rows in the table. If the target table has any IDENTITY columns, the EXPLICIT_IDS option is enabled to override 
the auto-generated values and load the incoming values. The following parameters (see Appendix B) can be set for bulk loader:

redshift.bulk.load.max.rows.before.flush:: When the max rows is reached, the flat file is sent to S3 and loaded into the database. The default is 100,000 rows.
redshift.bulk.load.max.bytes.before.flush:: When the max bytes is reached, the flat file is sent to S3 and loaded into the database. The default is 1,000,000,000 bytes.
redshift.bulk.load.s3.bucket:: The S3 bucket name where files are uploaded. This bucket should be created from the AWS console ahead of time.
redshift.bulk.load.s3.access.key:: The AWS access key ID to use as credentials for uploading to S3 and loading from S3.
redshift.bulk.load.s3.secret.key:: The AWS secret key to use as credentials for uploading to S3 and loading from S3.
redshift.bulk.load.s3.endpoint:: The AWS endpoint used for uploading to S3.  This is optional.  You might need to specify if you get warnings about retrying during the S3 upload.

To clean and organize tables after bulk changes, it is recommended to run a "vacuum" against individual tables or the entire 
database so that consistent query performance is maintained. Deletes and updates mark rows for delete that are not automatically 
reclaimed. New rows are stored in a separate unsorted region, forcing queries to sort on demand. Consider running a "vacuum" 
periodically during a maintenance window when there is minimal query activity that will be affected. If large batches are 
continually loaded from SymmetricDS, the "vacuum" command can be run after committing a batch by using a load filter (see Section 3.9) 
for the "batch commit" event, like this:

[source, JAVA]
----
for (String tablename : context.getParsedTables().keySet()) {
    engine.getSqlTemplate().update("vacuum " + tablename, new Object[] { } );
}
----